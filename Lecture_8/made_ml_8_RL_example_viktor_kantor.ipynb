{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение с подкреплением: пример"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перевернутый маятник: симуляция и бейзлайны"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='cart_pole.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим тестовую среду в Open AI Gym для задачи балансировки перевернутого маятника. Предварительно потребуется выполнить pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00530731, -0.04841013, -0.02095877, -0.0463515 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00627551, -0.24322537, -0.0218858 ,  0.23964575]), 1.0, False, {})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Описание состояния:*\n",
    "1. позиция тележки – значение в диапазоне [-4.8, 4.8];\n",
    "2. скорость тележки;\n",
    "3. угол отклонения шеста от вертикали – значение в диапазоне [-24°, 24°];\n",
    "4. скорость изменения угла наклона шеста.\n",
    "\n",
    "*Действие может принимать два значения – 0 и 1:*\n",
    "0 – толкнуть тележку влево (приложить к тележке горизонтальную силу, равную +1);\n",
    "1 – толкнуть тележку вправо (приложить к тележке горизонтальную силу, равную -1).\n",
    "Награда на каждом шаге равна 1, включая и последний шаг.\n",
    "\n",
    "Начальное состояние задается при помощи датчика равномерно распределенных случайных чисел.\n",
    "\n",
    "*Условия завершения эпизода:*\n",
    "1. угол шеста вышел из диапазона [-24°, 24°];\n",
    "1. позиция тележки вышла из допустимого диапазона [-4.8, 4.8];\n",
    "1. длина эпизода превышает 500;\n",
    "\n",
    "Успехом считается только третий вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_action(env, state):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "def simulate_games(action_function):\n",
    "    for episode in range(10):\n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in range(500):\n",
    "            env.render()\n",
    "            \n",
    "            action = action_function(env, state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            \n",
    "            print(t, state, reward, done, info, action)\n",
    "            if done:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [-0.0313689   0.24342843  0.0423398  -0.2855433 ] 1.0 False {} 1\n",
      "1 [-0.02650033  0.04772901  0.03662893  0.02018693] 1.0 False {} 0\n",
      "2 [-0.02554575  0.24230704  0.03703267 -0.26071784] 1.0 False {} 1\n",
      "3 [-0.02069961  0.04667655  0.03181832  0.04341182] 1.0 False {} 0\n",
      "4 [-0.01976608  0.24132813  0.03268655 -0.23906476] 1.0 False {} 1\n",
      "5 [-0.01493951  0.43596826  0.02790526 -0.52126086] 1.0 False {} 1\n",
      "6 [-0.00622015  0.63068652  0.01748004 -0.80502146] 1.0 False {} 1\n",
      "7 [ 0.00639358  0.43532935  0.00137961 -0.5068916 ] 1.0 False {} 0\n",
      "8 [ 0.01510017  0.24018798 -0.00875822 -0.21377423] 1.0 False {} 0\n",
      "9 [ 0.01990393  0.04519233 -0.01303371  0.07613314] 1.0 False {} 0\n",
      "10 [ 0.02080777 -0.14974037 -0.01151104  0.36467556] 1.0 False {} 0\n",
      "11 [ 0.01781297 -0.34469685 -0.00421753  0.65370674] 1.0 False {} 0\n",
      "12 [ 0.01091903 -0.14951643  0.0088566   0.35969875] 1.0 False {} 1\n",
      "13 [ 0.0079287  -0.34476315  0.01605058  0.65516116] 1.0 False {} 0\n",
      "14 [ 0.00103344 -0.1498683   0.0291538   0.36757521] 1.0 False {} 1\n",
      "15 [-0.00196393  0.04482752  0.0365053   0.08422559] 1.0 False {} 1\n",
      "16 [-0.00106738 -0.15079819  0.03818982  0.38819882] 1.0 False {} 0\n",
      "17 [-0.00408334 -0.34644082  0.04595379  0.69267387] 1.0 False {} 0\n",
      "18 [-0.01101216 -0.15198551  0.05980727  0.41480449] 1.0 False {} 1\n",
      "19 [-0.01405187 -0.34790189  0.06810336  0.7257265 ] 1.0 False {} 0\n",
      "20 [-0.02100991 -0.54389615  0.08261789  1.03904329] 1.0 False {} 0\n",
      "21 [-0.03188783 -0.3499634   0.10339876  0.77339842] 1.0 False {} 1\n",
      "22 [-0.0388871  -0.54634438  0.11886672  1.09674171] 1.0 False {} 0\n",
      "23 [-0.04981398 -0.74281371  0.14080156  1.42423076] 1.0 False {} 0\n",
      "24 [-0.06467026 -0.93936741  0.16928617  1.75740264] 1.0 False {} 0\n",
      "25 [-0.08345761 -0.74652004  0.20443423  1.52179939] 1.0 False {} 1\n",
      "26 [-0.09838801 -0.55436975  0.23487021  1.29926183] 1.0 True {} 1\n",
      "0 [-0.01227753 -0.18408736 -0.00556386  0.27058306] 1.0 False {} 0\n",
      "1 [-1.59592770e-02 -3.79129474e-01 -1.52197005e-04  5.61505937e-01] 1.0 False {} 0\n",
      "2 [-0.02354187 -0.57424929  0.01107792  0.85414091] 1.0 False {} 0\n",
      "3 [-0.03502685 -0.37928006  0.02816074  0.56496186] 1.0 False {} 1\n",
      "4 [-0.04261245 -0.57478556  0.03945998  0.86638199] 1.0 False {} 0\n",
      "5 [-0.05410816 -0.77042167  0.05678762  1.17120594] 1.0 False {} 0\n",
      "6 [-0.0695166  -0.96623418  0.08021174  1.48113782] 1.0 False {} 0\n",
      "7 [-0.08884128 -1.16223799  0.10983449  1.79775521] 1.0 False {} 0\n",
      "8 [-0.11208604 -0.96850329  0.1457896   1.54113071] 1.0 False {} 1\n",
      "9 [-0.13145611 -1.16504645  0.17661221  1.87552837] 1.0 False {} 0\n",
      "10 [-0.15475704 -1.36160202  0.21412278  2.21742994] 1.0 True {} 0\n",
      "0 [ 0.0460832  -0.22183239 -0.02422816  0.26080716] 1.0 False {} 0\n",
      "1 [ 0.04164656 -0.02637311 -0.01901202 -0.0394181 ] 1.0 False {} 1\n",
      "2 [ 0.04111909 -0.22121735 -0.01980038  0.24720624] 1.0 False {} 0\n",
      "3 [ 0.03669475 -0.02581829 -0.01485625 -0.05165583] 1.0 False {} 1\n",
      "4 [ 0.03617838  0.1695135  -0.01588937 -0.34898875] 1.0 False {} 1\n",
      "5 [ 0.03956865 -0.0253789  -0.02286914 -0.06135833] 1.0 False {} 0\n",
      "6 [ 0.03906107 -0.22016562 -0.02409631  0.2240224 ] 1.0 False {} 0\n",
      "7 [ 0.03465776 -0.41493504 -0.01961586  0.50900808] 1.0 False {} 0\n",
      "8 [ 0.02635906 -0.21954229 -0.0094357   0.21020866] 1.0 False {} 1\n",
      "9 [ 0.02196822 -0.02428671 -0.00523153 -0.08543573] 1.0 False {} 1\n",
      "10 [ 0.02148248  0.17090984 -0.00694024 -0.37976461] 1.0 False {} 1\n",
      "11 [ 0.02490068 -0.02411287 -0.01453554 -0.08927801] 1.0 False {} 0\n",
      "12 [ 0.02441842  0.17121438 -0.0163211  -0.38651123] 1.0 False {} 1\n",
      "13 [ 0.02784271 -0.02367212 -0.02405132 -0.09901859] 1.0 False {} 0\n",
      "14 [ 0.02736927 -0.21844126 -0.02603169  0.18598021] 1.0 False {} 0\n",
      "15 [ 0.02300044 -0.41318128 -0.02231209  0.47033881] 1.0 False {} 0\n",
      "16 [ 0.01473681 -0.21775138 -0.01290531  0.17070772] 1.0 False {} 1\n",
      "17 [ 0.01038179 -0.41268626 -0.00949116  0.45929163] 1.0 False {} 0\n",
      "18 [ 0.00212806 -0.21743144 -0.00030532  0.16363224] 1.0 False {} 1\n",
      "19 [-0.00222057 -0.41254902  0.00296732  0.45621883] 1.0 False {} 0\n",
      "20 [-0.01047155 -0.6077128   0.0120917   0.7498356 ] 1.0 False {} 0\n",
      "21 [-0.0226258  -0.80299942  0.02708841  1.04629899] 1.0 False {} 0\n",
      "22 [-0.03868579 -0.60824729  0.04801439  0.76224099] 1.0 False {} 1\n",
      "23 [-0.05085074 -0.80399658  0.06325921  1.06963734] 1.0 False {} 0\n",
      "24 [-0.06693067 -0.60976568  0.08465196  0.79745953] 1.0 False {} 1\n",
      "25 [-0.07912598 -0.41590092  0.10060115  0.53256165] 1.0 False {} 1\n",
      "26 [-0.087444   -0.61228319  0.11125238  0.85517257] 1.0 False {} 0\n",
      "27 [-0.09968966 -0.80873114  0.12835583  1.18066355] 1.0 False {} 0\n",
      "28 [-0.11586429 -0.61548745  0.1519691   0.93081561] 1.0 False {} 1\n",
      "29 [-0.12817404 -0.42270686  0.17058541  0.6894846 ] 1.0 False {} 1\n",
      "30 [-0.13662817 -0.61973388  0.18437511  1.03064475] 1.0 False {} 0\n",
      "31 [-0.14902285 -0.42747944  0.204988    0.80105056] 1.0 False {} 1\n",
      "32 [-0.15757244 -0.23566975  0.22100901  0.57920507] 1.0 True {} 1\n",
      "0 [ 0.04077569 -0.16800594  0.03962995  0.34578784] 1.0 False {} 0\n",
      "1 [0.03741557 0.02653054 0.04654571 0.0658604 ] 1.0 False {} 1\n",
      "2 [ 0.03794618  0.22095532  0.04786291 -0.21178141] 1.0 False {} 1\n",
      "3 [ 0.04236529  0.41536142  0.04362729 -0.48899029] 1.0 False {} 1\n",
      "4 [ 0.05067252  0.219652    0.03384748 -0.18288314] 1.0 False {} 0\n",
      "5 [ 0.05506556  0.41427369  0.03018982 -0.46469921] 1.0 False {} 1\n",
      "6 [ 0.06335103  0.60895632  0.02089583 -0.74771562] 1.0 False {} 1\n",
      "7 [ 0.07553016  0.41355242  0.00594152 -0.44853083] 1.0 False {} 0\n",
      "8 [ 0.0838012   0.60858982 -0.0030291  -0.73933498] 1.0 False {} 1\n",
      "9 [ 0.095973    0.80375347 -0.01781579 -1.03296966] 1.0 False {} 1\n",
      "10 [ 0.11204807  0.60887295 -0.03847519 -0.74593283] 1.0 False {} 0\n",
      "11 [ 0.12422553  0.41430243 -0.05339384 -0.46560196] 1.0 False {} 0\n",
      "12 [ 0.13251158  0.21997398 -0.06270588 -0.19021503] 1.0 False {} 0\n",
      "13 [ 0.13691106  0.41593435 -0.06651018 -0.50200134] 1.0 False {} 1\n",
      "14 [ 0.14522974  0.22180985 -0.07655021 -0.23099797] 1.0 False {} 0\n",
      "15 [ 0.14966594  0.41793744 -0.08117017 -0.5468116 ] 1.0 False {} 1\n",
      "16 [ 0.15802469  0.2240441  -0.0921064  -0.28076701] 1.0 False {} 0\n",
      "17 [ 0.16250557  0.42035095 -0.09772174 -0.60102014] 1.0 False {} 1\n",
      "18 [ 0.17091259  0.61669433 -0.10974215 -0.92281457] 1.0 False {} 1\n",
      "19 [ 0.18324648  0.81311415 -0.12819844 -1.24787138] 1.0 False {} 1\n",
      "20 [ 0.19950876  0.61984759 -0.15315587 -0.9979377 ] 1.0 False {} 0\n",
      "21 [ 0.21190571  0.427068   -0.17311462 -0.75700517] 1.0 False {} 0\n",
      "22 [ 0.22044707  0.6240993  -0.18825472 -1.09877647] 1.0 False {} 1\n",
      "23 [ 0.23292906  0.8211324  -0.21023025 -1.444125  ] 1.0 True {} 1\n",
      "0 [-0.00619573  0.17665227  0.03180804 -0.26769096] 1.0 False {} 1\n",
      "1 [-0.00266268 -0.01890885  0.02645422  0.03485233] 1.0 False {} 0\n",
      "2 [-0.00304086  0.17582395  0.02715127 -0.24936803] 1.0 False {} 1\n",
      "3 [ 4.75617969e-04  3.70547847e-01  2.21639044e-02 -5.33364734e-01] 1.0 False {} 1\n",
      "4 [ 0.00788657  0.56535119  0.01149661 -0.81898233] 1.0 False {} 1\n",
      "5 [ 0.0191936   0.37007379 -0.00488304 -0.52270565] 1.0 False {} 0\n",
      "6 [ 0.02659507  0.1750209  -0.01533715 -0.23156542] 1.0 False {} 0\n",
      "7 [ 0.03009549  0.37035861 -0.01996846 -0.52904647] 1.0 False {} 1\n",
      "8 [ 0.03750266  0.56575571 -0.03054939 -0.82795402] 1.0 False {} 1\n",
      "9 [ 0.04881778  0.37106447 -0.04710847 -0.54503355] 1.0 False {} 0\n",
      "10 [ 0.05623907  0.56681562 -0.05800914 -0.85217929] 1.0 False {} 1\n",
      "11 [ 0.06757538  0.37253045 -0.07505272 -0.57828685] 1.0 False {} 0\n",
      "12 [ 0.07502599  0.17853617 -0.08661846 -0.3101594 ] 1.0 False {} 0\n",
      "13 [ 0.07859671  0.37477857 -0.09282165 -0.62885342] 1.0 False {} 1\n",
      "14 [ 0.08609228  0.18106619 -0.10539872 -0.3667861 ] 1.0 False {} 0\n",
      "15 [ 0.08971361  0.37751564 -0.11273444 -0.69075493] 1.0 False {} 1\n",
      "16 [ 0.09726392  0.18412352 -0.12654954 -0.43558193] 1.0 False {} 0\n",
      "17 [ 0.10094639 -0.0090011  -0.13526118 -0.18531787] 1.0 False {} 0\n",
      "18 [ 0.10076637  0.18777078 -0.13896753 -0.51742541] 1.0 False {} 1\n",
      "19 [ 0.10452179  0.38454772 -0.14931604 -0.85047036] 1.0 False {} 1\n",
      "20 [ 0.11221274  0.19174272 -0.16632545 -0.60821683] 1.0 False {} 0\n",
      "21 [ 0.11604759  0.3887514  -0.17848979 -0.94832624] 1.0 False {} 1\n",
      "22 [ 0.12382262  0.58576894 -0.19745631 -1.29135532] 1.0 False {} 1\n",
      "23 [ 0.135538    0.39362767 -0.22328342 -1.06641938] 1.0 True {} 0\n",
      "0 [-0.01693465  0.16831049  0.04848835 -0.23432483] 1.0 False {} 1\n",
      "1 [-0.01356844  0.36270731  0.04380185 -0.51132736] 1.0 False {} 1\n",
      "2 [-0.0063143   0.16699661  0.03357531 -0.20516925] 1.0 False {} 0\n",
      "3 [-0.00297437 -0.02858899  0.02947192  0.09791309] 1.0 False {} 0\n",
      "4 [-0.00354615  0.16609844  0.03143018 -0.18532775] 1.0 False {} 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [-2.24177575e-04  3.60756918e-01  2.77236283e-02 -4.67932306e-01] 1.0 False {} 1\n",
      "6 [ 0.00699096  0.55547645  0.01836498 -0.75174966] 1.0 False {} 1\n",
      "7 [ 0.01810049  0.75034039  0.00332999 -1.03859729] 1.0 False {} 1\n",
      "8 [ 0.0331073   0.94541794 -0.01744196 -1.33023296] 1.0 False {} 1\n",
      "9 [ 0.05201566  1.14075552 -0.04404662 -1.62832245] 1.0 False {} 1\n",
      "10 [ 0.07483077  1.3363667  -0.07661307 -1.93440015] 1.0 False {} 1\n",
      "11 [ 0.1015581   1.14214303 -0.11530107 -1.66642145] 1.0 False {} 0\n",
      "12 [ 0.12440096  0.94853587 -0.1486295  -1.41176243] 1.0 False {} 0\n",
      "13 [ 0.14337168  1.14515471 -0.17686475 -1.74697546] 1.0 False {} 1\n",
      "14 [ 0.16627477  0.95242936 -0.21180425 -1.51412473] 1.0 True {} 0\n",
      "0 [-0.02635413  0.17009048  0.04691558 -0.28134939] 1.0 False {} 1\n",
      "1 [-0.02295233  0.36451291  0.0412886  -0.55887402] 1.0 False {} 1\n",
      "2 [-0.01566207  0.55903173  0.03011112 -0.83826817] 1.0 False {} 1\n",
      "3 [-0.00448143  0.75372983  0.01334575 -1.12133161] 1.0 False {} 1\n",
      "4 [ 0.01059316  0.94867424 -0.00908088 -1.40979864] 1.0 False {} 1\n",
      "5 [ 0.02956665  0.75366608 -0.03727685 -1.11996821] 1.0 False {} 0\n",
      "6 [ 0.04463997  0.55905241 -0.05967622 -0.83920736] 1.0 False {} 0\n",
      "7 [ 0.05582102  0.75493621 -0.07646036 -1.15004442] 1.0 False {} 1\n",
      "8 [ 0.07091974  0.56089083 -0.09946125 -0.8822842 ] 1.0 False {} 0\n",
      "9 [ 0.08213756  0.3672502  -0.11710694 -0.62245219] 1.0 False {} 0\n",
      "10 [ 0.08948256  0.56379583 -0.12955598 -0.94960217] 1.0 False {} 1\n",
      "11 [ 0.10075848  0.37063343 -0.14854802 -0.70026981] 1.0 False {} 0\n",
      "12 [ 0.10817115  0.56746829 -0.16255342 -1.03578316] 1.0 False {} 1\n",
      "13 [ 0.11952051  0.76433396 -0.18326908 -1.37476934] 1.0 False {} 1\n",
      "14 [ 0.13480719  0.9612113  -0.21076447 -1.71871974] 1.0 True {} 1\n",
      "0 [-0.04368491  0.15100358 -0.01411713 -0.26172516] 1.0 False {} 1\n",
      "1 [-0.04066484  0.34632417 -0.01935163 -0.55882715] 1.0 False {} 1\n",
      "2 [-0.03373836  0.54171233 -0.03052817 -0.85754355] 1.0 False {} 1\n",
      "3 [-0.02290411  0.3470193  -0.04767904 -0.57461397] 1.0 False {} 0\n",
      "4 [-0.01596372  0.54277611 -0.05917132 -0.88192782] 1.0 False {} 1\n",
      "5 [-0.0051082   0.34850562 -0.07680988 -0.60841829] 1.0 False {} 0\n",
      "6 [ 0.00186191  0.54461261 -0.08897825 -0.92427137] 1.0 False {} 1\n",
      "7 [ 0.01275416  0.35079793 -0.10746367 -0.66082454] 1.0 False {} 0\n",
      "8 [ 0.01977012  0.1573225  -0.12068016 -0.40381907] 1.0 False {} 0\n",
      "9 [ 0.02291657  0.35393077 -0.12875654 -0.73198049] 1.0 False {} 1\n",
      "10 [ 0.02999519  0.1608009  -0.14339615 -0.4824336 ] 1.0 False {} 0\n",
      "11 [ 0.03321121  0.35762473 -0.15304483 -0.81665329] 1.0 False {} 1\n",
      "12 [ 0.0403637   0.1648921  -0.16937789 -0.57575322] 1.0 False {} 0\n",
      "13 [ 0.04366154  0.36193279 -0.18089296 -0.91664406] 1.0 False {} 1\n",
      "14 [ 0.0509002   0.55897864 -0.19922584 -1.26028313] 1.0 False {} 1\n",
      "15 [ 0.06207977  0.36688274 -0.2244315  -1.03602444] 1.0 True {} 0\n",
      "0 [-0.01237429  0.18271676  0.00053519 -0.27831619] 1.0 False {} 1\n",
      "1 [-0.00871996  0.37783107 -0.00503114 -0.57083027] 1.0 False {} 1\n",
      "2 [-0.00116334  0.57302321 -0.01644774 -0.86509393] 1.0 False {} 1\n",
      "3 [ 0.01029713  0.76836513 -0.03374962 -1.16290259] 1.0 False {} 1\n",
      "4 [ 0.02566443  0.57369853 -0.05700767 -0.88098947] 1.0 False {} 0\n",
      "5 [ 0.0371384   0.37939539 -0.07462746 -0.60675942] 1.0 False {} 0\n",
      "6 [ 0.04472631  0.57547711 -0.08676265 -0.92198346] 1.0 False {} 1\n",
      "7 [ 0.05623585  0.77165748 -0.10520232 -1.24062333] 1.0 False {} 1\n",
      "8 [ 0.071669    0.57803171 -0.13001479 -0.98266289] 1.0 False {} 0\n",
      "9 [ 0.08322963  0.38486886 -0.14966804 -0.7334808 ] 1.0 False {} 0\n",
      "10 [ 0.09092701  0.1920968  -0.16433766 -0.49139364] 1.0 False {} 0\n",
      "11 [ 0.09476895 -0.00037212 -0.17416553 -0.25467806] 1.0 False {} 0\n",
      "12 [ 0.09476151  0.19675333 -0.17925909 -0.5968391 ] 1.0 False {} 1\n",
      "13 [ 0.09869657  0.00453267 -0.19119588 -0.36554868] 1.0 False {} 0\n",
      "14 [ 0.09878723  0.20178432 -0.19850685 -0.71190432] 1.0 False {} 1\n",
      "15 [ 0.10282291  0.39902019 -0.21274494 -1.05992666] 1.0 True {} 1\n",
      "0 [-0.01895149 -0.18168993 -0.00135691  0.3372403 ] 1.0 False {} 0\n",
      "1 [-0.02258529 -0.37679255  0.0053879   0.62949502] 1.0 False {} 0\n",
      "2 [-0.03012114 -0.57198927  0.0179778   0.92386989] 1.0 False {} 0\n",
      "3 [-0.04156093 -0.76734941  0.03645519  1.22214793] 1.0 False {} 0\n",
      "4 [-0.05690792 -0.96292159  0.06089815  1.52602675] 1.0 False {} 0\n",
      "5 [-0.07616635 -0.76858525  0.09141869  1.2529556 ] 1.0 False {} 1\n",
      "6 [-0.09153805 -0.57474547  0.1164778   0.99024974] 1.0 False {} 1\n",
      "7 [-0.10303296 -0.77121747  0.13628279  1.31712794] 1.0 False {} 0\n",
      "8 [-0.11845731 -0.57805684  0.16262535  1.07001674] 1.0 False {} 1\n",
      "9 [-0.13001845 -0.77491172  0.18402569  1.40900438] 1.0 False {} 0\n",
      "10 [-0.14551668 -0.58248683  0.21220578  1.17903937] 1.0 True {} 1\n"
     ]
    }
   ],
   "source": [
    "simulate_games(get_random_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cartpole_random.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cartpole_random.py\n",
    "import gym\n",
    "import random\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "def get_random_action(env, state):\n",
    "    return env.action_space.sample()\n",
    "\n",
    "def get_balancing_action(env, state):\n",
    "    return int(state[2] > 0)\n",
    "\n",
    "def get_tricky_balancing_action(env, state):\n",
    "    next_action = int(state[2] > 0)\n",
    "    if (abs(state[3]) > 1.0 and state[2] * state [3] < 0):\n",
    "        next_action = 1 - next_action\n",
    "    return next_action\n",
    "\n",
    "def simulate_games(action_function):\n",
    "    for episode in range(10):\n",
    "        state = env.reset()\n",
    "        \n",
    "        for t in range(500):\n",
    "            env.render()\n",
    "            \n",
    "            action = action_function(env, state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            \n",
    "            print(t, state, reward, done, info, action)\n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "simulate_games(get_random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучаем решение с помощью Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой секции мы решим задачу с помощью Q-learning, применив для оценки Q-функции двухслойную нейросеть. Строго говоря, в такой конфигурации это немного громко будет называть \"Deep Q-learning Network\", но основная идея передана верно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Примечания*\n",
    "\n",
    "1. Для того, чтобы пример работал, потребуются tensorflow 2, keras и ffmpeg (для записи видео с симуляцией). Первое и второе ставим через pip install, второе - через upt-get install в Linux, brew install на Mac. Если у вас Windows и установлена Anaconda, можно воспользоваться conda install в Anaconda promt.\n",
    "\n",
    "2. Ссылка на источник изначальной версии кода: http://www.100byte.ru/python/cartPole/cartPole.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing cartpole_dqnn.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cartpole_dqnn.py\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "# Агент с Q-обучением\n",
    "class DQNAgent():\n",
    "    def __init__(self, observation_space, action_space):\n",
    "        self.state_size = observation_space\n",
    "        self.action_size = action_space\n",
    "        self.memory = deque(maxlen = 20000) # Тип collections.deque\n",
    "        self.alpha = 1.0 # Скорость обучения агента\n",
    "        self.gamma = 0.95 # Коэффициент уменьшения вознаграждения агента\n",
    "        # Уровень обучения повышается с коэффициентом exploration_decay\n",
    "        # Влияет на выбор действия action (0 или 1)\n",
    "        self.exploration_rate = 1.0\n",
    "        self.exploration_min = 0.01\n",
    "        self.exploration_decay = 0.995\n",
    "        self.learning_rate = 0.001 # Скорость обучения сети\n",
    "        self.model = self.build_model()\n",
    "        print(self.model.summary())\n",
    "\n",
    "    # Создает модель сети\n",
    "    def build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim = self.state_size, activation = 'relu'))\n",
    "        model.add(Dense(24, activation = 'relu'))\n",
    "        model.add(Dense(2, activation = 'linear'))\n",
    "        model.compile(loss = 'mse', optimizer = Adam(lr = self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    # Запоминаем историю\n",
    "    def remember(self, state, action, reward, state_next, done):\n",
    "        self.memory.append((state, action, reward, state_next, done))\n",
    "    #\n",
    "    # Определяет и возвращает действие\n",
    "    def findAction(self, state):\n",
    "        # Случайный выбор действия - 0 или 1\n",
    "        if np.random.rand() <= self.exploration_rate:\n",
    "            return random.randrange(self.action_size) # или random.randint(0, 1)\n",
    "        # Выбор действия по состоянию объекта\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0]) # Возвращает действие\n",
    "    #\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size: return\n",
    "        # Обучение агента\n",
    "        \n",
    "        # Случайная выборка batch_size элементов для обучения агента\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, state_next, done in minibatch:\n",
    "            \n",
    "            # Пример (done = False):\n",
    "            # state: [[-0.00626842 0.41118423 -0.07340495 -0.77232979]]\n",
    "            # q_values (до корректировки): [[0.052909 0.05275263]] - numpy.ndarray\n",
    "            # state_next: [[ 0.00195526 0.21714493 -0.08885155 -0.50361631]]\n",
    "            # q_values_next: [[0.03970249 0.02732118]]\n",
    "            # Qsa = 1.0377173654735088\n",
    "            # reward = 1.0\n",
    "            # action = 0\n",
    "            # q_values (после корректировки): [[1.0377173 0.05275263]]\n",
    "            # q_values (после обучения НС): [[0.07063997 0.04742151]]\n",
    "            \n",
    "            q_values = self.model.predict(state)\n",
    "            \n",
    "            if done:\n",
    "                Qsa = reward\n",
    "                \n",
    "            else:\n",
    "                q_values_next = self.model.predict(state_next)[0]\n",
    "                # Текущая оценка полезности действия action\n",
    "                Qsa = q_values[0][action]\n",
    "                \n",
    "                # Уточненная оценка полезности действия action\n",
    "                Qsa = Qsa + self.alpha * (reward + self.gamma * np.amax(q_values_next) - Qsa)\n",
    "                \n",
    "            # Формируем цель обучения сети\n",
    "            q_values[0][action] = Qsa\n",
    "            \n",
    "            # Обучение сети\n",
    "            self.model.fit(state, q_values, epochs = 1, verbose = 0)\n",
    "            \n",
    "        if self.exploration_rate > self.exploration_min: \n",
    "            self.exploration_rate *= self.exploration_decay\n",
    "            \n",
    "def make_video(env, agent):\n",
    "    env = gym.wrappers.Monitor(env, os.path.join(os.getcwd(), \"cartpole_videos\"), force=True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    while not done:\n",
    "        env.render()\n",
    "        state = np.reshape(state, (1, env.observation_space.shape[0]))\n",
    "        action = agent.findAction(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(\"Testing steps: {} rewards {}: \".format(steps, rewards))\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    env = gym.make('CartPole-v1') # Создаем среду\n",
    "    observation_space = env.observation_space.shape[0] # 4\n",
    "    action_space = env.action_space.n # 2\n",
    "    \n",
    "    # DQN - глубокая Q-нейронная сеть\n",
    "    dqn_agent = DQNAgent(observation_space, action_space) # Создаем агента\n",
    "    episodes = 1001 # Число игровых эпизодов + 1\n",
    "    \n",
    "    # scores - хранит длительность игры в последних 100 эпизодах\n",
    "    # После достижения maxlen новые значения, добавляемые в scores, будут вытеснять прежние\n",
    "    scores = deque(maxlen = 100) # Тип collections.deque.\n",
    "    fail = True\n",
    "    seed = 2\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    env.seed(seed)\n",
    "    \n",
    "    for e in range(episodes):\n",
    "        # Получаем начальное состояние объекта перед началом каждой игры (каждого эпизода)\n",
    "        state = env.reset() # Как вариант: state = [0.0364131 -0.02130403 -0.03887796 -0.01044108]\n",
    "        # state[0] - позиция тележки\n",
    "        # state[1] - скорость тележки\n",
    "        # state[2] - угол отклонения шеста от вертикали в радианах\n",
    "        # state[3] - скорость изменения угла наклона шеста\n",
    "        state = np.reshape(state, (1, observation_space))\n",
    "        \n",
    "        # Начинаем игру\n",
    "        # frame - текущий кадр (момент) игры\n",
    "        # Цель - как можно дольше не допустить падения шеста\n",
    "        frames = 0\n",
    "        while True:\n",
    "            frames += 1\n",
    "            action = dqn_agent.findAction(state) # Определяем очередное действие\n",
    "            \n",
    "            # Получаем от среды, в которой выполнено действие action, состояние объекта, награду и значение флага завершения игры\n",
    "            # В каждый момент игры, пока не наступило одно из условий ее прекращения, награда равна 1\n",
    "            state_next, reward, done, info = env.step(action)\n",
    "            state_next = np.reshape(state_next, (1, observation_space))\n",
    "            \n",
    "            reward = reward if not done else -reward\n",
    "            #reward = -100 * (abs(state_next[0, 2]) - abs(state[0, 2]))\n",
    "            \n",
    "            # Запоминаем предыдущее состояние объекта, действие, награду за это действие, текущее состояние и значение done\n",
    "            dqn_agent.remember(state, action, reward, state_next, done)\n",
    "            state = state_next # Обновляем текущее состояние\n",
    "            \n",
    "            # done становится равным True, когда завершается игра, например, отклонение угла превысило допустимое значение\n",
    "            if done:\n",
    "                # Печатаем продолжительность игры и покидаем внутренний цикл while\n",
    "                print(\"Эпизод: {}/{}, продолжительность игры в кадрах: {}\".format(e, episodes - 1, frames))\n",
    "                break\n",
    "        scores.append(frames)\n",
    "        \n",
    "        if e > 100:\n",
    "            score_mean = np.mean(scores)\n",
    "            if score_mean > 1950:\n",
    "                print('Цель достигнута. Средняя продолжительность игры: ', score_mean)\n",
    "                fail = False\n",
    "                break\n",
    "        # Продолжаем обучать агента\n",
    "        dqn_agent.replay(24)\n",
    "    if fail:\n",
    "        print('Задача не решена ')\n",
    "        \n",
    "    make_video(env, dqn_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также существуют библиотеки с готовыми бейзлайнами для агентов:\n",
    "https://github.com/openai/baselines - вариант непосредственно от Open AI\n",
    "https://stable-baselines.readthedocs.io/en/master/ - несколько расширенный форк\n",
    "\n",
    "Есть сложность, что большинство решений реализовано для tensorflow 1.x. \n",
    "У stable baselines есть экспериментальная версия для tf 2: https://github.com/Stable-Baselines-Team/stable-baselines-tf2\n",
    "А если вы запускаете код в Google Colab, то для использования первой версии tf достаточно применить соответствующий jupyter notebook magic:\n",
    "%tensorflow_version 1.x\n",
    "(ссылка: https://colab.research.google.com/notebooks/tensorflow_version.ipynb )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Опциональное задание\n",
    "1. Поэкспериментируйте с custom reward в приведенном решении, попробовав ускорить за счет его выбора сходимость\n",
    "1. Попробуйте существенно ускорить сходимость решения (любые подходы приемлемы)\n",
    "1. Примените в этой задаче PG и PPO2, сравните сходимость с нашим Q-learning\n",
    "1. Попробуйте запустить и отрендерить симуляцию в Google Collab\n",
    "1. Поэкспериментируйте с архитектурой сети и добавлением дополнительных признаков\n",
    "1. Попытайтесь решить задачу без применения нейросетей - приближая Q-функцию другими моделями или вовсе применив табличные policy/value iteration после дискретизации пространства состояний"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
